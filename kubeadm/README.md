# 一、防火墙配置
```
chattr -i /etc/passwd* && chattr -i /etc/group* && chattr -i /etc/shadow* && chattr -i /etc/gshadow*

yum install iptables iptables-services -y

cat > /etc/sysconfig/iptables << \EOF
# Generated by iptables-save v1.4.21 on Thu Aug  1 01:26:09 2019
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:RH-Firewall-1-INPUT - [0:0]
-A INPUT -j RH-Firewall-1-INPUT
-A FORWARD -j RH-Firewall-1-INPUT
-A RH-Firewall-1-INPUT -i lo -j ACCEPT
-A RH-Firewall-1-INPUT -p icmp -m icmp --icmp-type any -j ACCEPT
-A RH-Firewall-1-INPUT -s 192.168.56.0/24 -p tcp -m tcp --dport 22 -j ACCEPT
-A RH-Firewall-1-INPUT -p tcp -m tcp --dport 22 -j DROP
### k8s ###
-A RH-Firewall-1-INPUT -s 192.168.56.11/32 -j ACCEPT
-A RH-Firewall-1-INPUT -s 192.168.56.12/32 -j ACCEPT
-A RH-Firewall-1-INPUT -s 192.168.56.13/32 -j ACCEPT
# serviceSubnet rules
-A RH-Firewall-1-INPUT -s 10.96.0.0/12 -j ACCEPT
# podSubnet rules
-A RH-Firewall-1-INPUT -s 10.244.0.0/16 -j ACCEPT
# keepalived rules
-A RH-Firewall-1-INPUT -p vrrp -j ACCEPT
# port rules
-A RH-Firewall-1-INPUT -s 192.168.56.1/32 -p tcp -m multiport --dports 80,443,1080,6443,16443,30000:32767 -j ACCEPT
### k8s ###
-A RH-Firewall-1-INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A RH-Firewall-1-INPUT -j REJECT --reject-with icmp-host-prohibited
COMMIT
# Completed on Thu Aug  1 01:26:09 2019
EOF
systemctl restart iptables.service
systemctl enable iptables.service

iptables -nvL
```
# 二、初始化
```bash
cat > /etc/hosts << \EOF
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.56.11 linux-node1 linux-node1.example.com
192.168.56.12 linux-node2 linux-node2.example.com
192.168.56.13 linux-node3 linux-node3.example.com
EOF

systemctl stop firewalld
systemctl disable firewalld

setenforce 0
sed -i 's/SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config
sed -i 's/SELINUXTYPE=.*/SELINUXTYPE=disabled/g' /etc/selinux/config

# 关闭 swap
swapoff -a
#sed -ir 's/.*swap.*/#&/' /etc/fstab
#或
yes | cp /etc/fstab /etc/fstab_bak
cat /etc/fstab_bak |grep -v swap > /etc/fstab

#export Time=`date "+%Y%m%d%H%M%S"`
#cp /etc/fstab /etc/fstab_$Time

cat > /etc/sysctl.d/k8s.conf << \EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
EOF

#加载 br_netfilter 模块
modprobe br_netfilter
sysctl -p /etc/sysctl.d/k8s.conf

#创建/etc/sysconfig/modules/ipvs.modules文件,保证在节点重启后能自动加载所需模块
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF

chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4

yum install -y ipset ipvsadm

yum install chrony -y
systemctl enable chronyd
systemctl restart chronyd
chronyc sources

yum install -y yum-utils \
  device-mapper-persistent-data \
  lvm2
  
yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo
    
#yum list docker-ce --showduplicates | sort -r

yum install -y docker-ce-18.09.9-3.el7.x86_64
systemctl start docker
systemctl enable docker
cat > /etc/docker/daemon.json << \EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "registry-mirrors" : [
    "https://ot2k4d59.mirror.aliyuncs.com/"
  ]
}
EOF
systemctl daemon-reload
systemctl restart docker

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
yum install -y kubelet-1.15.3-0 kubeadm-1.15.3-0 kubectl-1.15.3-0 --disableexcludes=kubernetes
systemctl daemon-reload
systemctl restart kubelet.service
kubeadm version
systemctl enable kubelet.service
systemctl status kubelet

#查看kubelet日志
journalctl -f -u kubelet

#kubelet.service服务位置
/lib/systemd/system/kubelet.service
```

# 三、初始化集群
1、命令行初始化
```bash
kubeadm init \
  --apiserver-advertise-address=192.168.56.11 \
  --image-repository registry.aliyuncs.com/google_containers \
  --kubernetes-version v1.15.3 \
  --apiserver-bind-port=6443 \
  --service-cidr=10.96.0.0/12 \
  --pod-network-cidr=10.244.0.0/16    #这里使用这个是因为官方flannel使用的这个段地址，不然的话,kube-flannel.yml那里需要调整

#获取加入集群的指令
kubeadm token create --print-join-command

kubeadm join 192.168.56.11:6443 --token 5avfk1.fwui1smk5utcu7m9     --discovery-token-ca-cert-hash sha256:6730e91a516d8bf3e26d8f5eddd6409a224f8703b94f6ecde2b1fd7481bbbd25

#集群初始化如果遇到问题，可以使用下面的命令进行清理
yes | kubeadm reset
ifconfig cni0 down
ip link delete cni0
ifconfig flannel.1 down
ip link delete flannel.1
rm -rf /var/lib/cni/
rm -f $HOME/.kube/config

systemctl restart kubelet
systemctl status kubelet
journalctl -f -u kubelet
```
2、通过配置文件进行初始化
```bash
#在 master 节点配置 kubeadm 初始化文件，可以通过如下命令导出默认的初始化配置：
root># kubeadm config print init-defaults > kubeadm.yaml
```
```
#然后根据我们自己的需求修改配置，比如修改 imageRepository 的值，kube-proxy 的模式为 ipvs

如果是 flannel 网络插件的，需要将 networking.podSubnet 设置为默认的 10.244.0.0/16

如果是 Calico 网络插件的，配置成 Calico 的默认网段 podSubnet: 192.168.0.0/16，这个也可以修改Calico的配置文件调整

rm -f kubeadm.yaml

cat > kubeadm.yaml << \EOF
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.56.11 #修改为主节点 IP
  bindPort: 6443
  #controlPlaneEndpoint: 1.1.1.100 #如果前面配置了负载均衡，此处填写vip地址
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: linux-node1.example.com
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS #dns 类型
etcd:
  local:
    dataDir: /var/lib/etcd
#imageRepository: k8s.gcr.io
imageRepository: registry.aliyuncs.com/google_containers #国内不能访问 Google，修改为阿里云
kind: ClusterConfiguration
kubernetesVersion: v1.15.3 # 修改版本号
networking:
  dnsDomain: cluster.local
  # 配置成 flannel 的默认网段
  serviceSubnet: 10.96.0.0/12
  podSubnet: 10.244.0.0/16
scheduler: {}
---
# 开启 IPVS 模式
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs # kube-proxy 模式
EOF

kubeadm init --config kubeadm.yaml
```
3、初始化进行的操作
```bash
初始化操作主要经历了下面15个步骤，每个阶段均输出均使用[步骤名称]作为开头：

    1、[init]：指定版本进行初始化操作
    2、[preflight] ：初始化前的检查和下载所需要的Docker镜像文件。
    3、[kubelet-start] ：生成kubelet的配置文件”/var/lib/kubelet/config.yaml”，没有这个文件kubelet无法启动，所以初始化之前的kubelet实际上启动失败。
    4、[certificates]：生成Kubernetes使用的证书，存放在/etc/kubernetes/pki目录中。
    5、[kubeconfig] ：生成 KubeConfig 文件，存放在/etc/kubernetes目录中，组件之间通信需要使用对应文件。
    6、[control-plane]：使用/etc/kubernetes/manifest目录下的YAML文件，安装 Master 组件。
    7、[etcd]：使用/etc/kubernetes/manifest/etcd.yaml安装Etcd服务。
    8、[wait-control-plane]：等待control-plan部署的Master组件启动。
    9、[apiclient]：检查Master组件服务状态。
    10、[uploadconfig]：更新配置
    11、[kubelet]：使用configMap配置kubelet。
    12、[patchnode]：更新CNI信息到Node上，通过注释的方式记录。
    13、[mark-control-plane]：为当前节点打标签，打了角色Master，和不可调度标签，这样默认就不会使用Master节点来运行Pod。
    14、[bootstrap-token]：生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到
    15、[addons]：安装附加组件CoreDNS和kube-proxy
    
kubectl默认会在执行的用户家目录下面的.kube目录下寻找config文件。这里是将在初始化时[kubeconfig]步骤生成的admin.conf拷贝到.kube/config。
```

2、单独部署coredns（选择操作）
```
# 不依赖kubeadm的方式，适用于不是使用kubeadm创建的k8s集群，或者kubeadm初始化集群之后，删除了dns相关部署
# 在calico网络中也配置一个coredns # 10.96.0.10 为k8s官方指定的kube-dns地址
rm -f coredns.yaml.sed deploy.sh coredns.yml
wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed
wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/deploy.sh
chmod +x deploy.sh
./deploy.sh -i 10.10.0.10 > coredns.yml  #这里从--service-cidr=10.10.0.0/16中选用10.10.0.10作为coredns地址
kubectl apply -f coredns.yml

# 查看
kubectl get pods --namespace kube-system
kubectl get svc --namespace kube-system

#删除coredns
kubectl delete deployment coredns -n kube-system
kubectl delete svc kube-dns -n kube-system
kubectl delete cm coredns -n kube-system
```
3、集群移除节点
```
1、#移除work节点
在准备移除的 worker 节点上执行
kubeadm reset

2、在第一个 master 节点 demo-master-a-1 上执行
kubectl delete node demo-worker-x-x
#worker 节点的名字可以通过在第一个 master 节点 demo-master-a-1 上执行 kubectl get nodes 命令获得
```

4、kube-proxy开启ipvs
```
1、#修改ConfigMap的kube-system/kube-proxy中的config.conf，把 mode: "" 改为mode: “ipvs" 保存退出即可

root># kubectl edit cm kube-proxy -n kube-system
configmap/kube-proxy edited

2、#删除之前的proxy pod
root># kubectl get pod -n kube-system |grep kube-proxy |awk '{system("kubectl delete pod "$1" -n kube-system")}'

3、#查看proxy运行状态
root># kubectl get pod -n kube-system | grep kube-proxy

4、#查看日志,如果有 `Using ipvs Proxier.` 说明kube-proxy的ipvs 开启成功!
root># kubectl logs kube-proxy-54qnw -n kube-system
I0518 20:24:09.319160       1 server_others.go:176] Using ipvs Proxier.
W0518 20:24:09.319751       1 proxier.go:386] IPVS scheduler not specified, use rr by default
I0518 20:24:09.320035       1 server.go:562] Version: v1.14.2
I0518 20:24:09.334372       1 conntrack.go:52] Setting nf_conntrack_max to 131072
I0518 20:24:09.334853       1 config.go:102] Starting endpoints config controller
I0518 20:24:09.334916       1 controller_utils.go:1027] Waiting for caches to sync for endpoints config controller
I0518 20:24:09.334945       1 config.go:202] Starting service config controller
I0518 20:24:09.334976       1 controller_utils.go:1027] Waiting for caches to sync for service config controller
I0518 20:24:09.435153       1 controller_utils.go:1034] Caches are synced for service config controller
I0518 20:24:09.435271       1 controller_utils.go:1034] Caches are synced for endpoints config controller
```

# 四、Master操作
```
#将 master 节点上面的 $HOME/.kube/config 文件拷贝到 node 节点对应的文件中
mkdir -p $HOME/.kube
yes | cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

scp $HOME/.kube/config root@linux-node2:$HOME/.kube/config
scp $HOME/.kube/config root@linux-node3:$HOME/.kube/config

#指令补全
yum install bash-completion -y
source <(kubectl completion bash)
echo "source <(kubectl completion bash)" >> ~/.bashrc
```

# 五、Node操作
```
#node节点操作
mkdir -p $HOME/.kube
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#加入集群
kubeadm join 192.168.56.11:6443 --token 5avfk1.fwui1smk5utcu7m9     --discovery-token-ca-cert-hash sha256:6730e91a516d8bf3e26d8f5eddd6409a224f8703b94f6ecde2b1fd7481bbbd25
```

# 六、集群操作
```
#批量重启docker
docker restart `docker ps -a -q` 

root># kubectl get nodes
NAME                      STATUS     ROLES    AGE     VERSION
linux-node1.example.com   NotReady   master   11m     v1.15.3
linux-node2.example.com   NotReady   <none>   5m9s    v1.15.3
linux-node3.example.com   NotReady   <none>   4m58s   v1.15.3

可以看到是 NotReady 状态，这是因为还没有安装网络插件，接下来安装网络插件，可以在文档 https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ 中选择我们自己的网络插件，这里我们安装 flannel:

iptables -I RH-Firewall-1-INPUT -s 10.96.0.0/16 -j ACCEPT
service iptables save

root># kubectl get pods -n kube-system
NAME                                              READY   STATUS    RESTARTS   AGE
coredns-5c98db65d4-mk254                          1/1     Running   0          14m
coredns-5c98db65d4-ntz98                          1/1     Running   0          14m
etcd-linux-node1.example.com                      1/1     Running   0          13m
kube-apiserver-linux-node1.example.com            1/1     Running   0          13m
kube-controller-manager-linux-node1.example.com   1/1     Running   0          13m
kube-flannel-ds-amd64-6kx7m                       1/1     Running   0          11m
kube-flannel-ds-amd64-cqfnb                       1/1     Running   0          11m
kube-flannel-ds-amd64-thxx2                       1/1     Running   0          11m
kube-proxy-gdtjg                                  1/1     Running   0          12m
kube-proxy-lcscl                                  1/1     Running   0          14m
kube-proxy-sb7d8                                  1/1     Running   0          12m
kube-scheduler-linux-node1.example.com            1/1     Running   0          13m
kubernetes-dashboard-fcfb4cbc-dqbq9               1/1     Running   0          4m43s

kubectl describe pod/coredns-5c98db65d4-mk254 -n kube-system

#创建Deployment
kubectl run --image=nginx nginx-web-1 --image-pull-policy='IfNotPresent' --replicas=3

#以不同方式暴露出去
kubectl expose deployment nginx-web-1 --port=80 --target-port=80
kubectl expose deployment nginx-web-1 --port=80 --target-port=80 --type=NodePort

root># kubectl exec -it nginx-web-1-5cc49f46bc-kn46r -- \
               sh -c "echo hello>/usr/share/nginx/html/index.html"

root># kubectl get svc -A
default       nginx-web-1   NodePort    10.10.43.53   <none>        80:30163/TCP             101s

root># kubectl get endpoints
nginx-web-1   10.244.154.193:80,10.244.44.193:80,10.244.89.129:80   5m27s

root># curl 10.10.43.53   
hello

#显示iptables规则(注意这里kube-proxy需要使用ipvs模式，上面主机预设的iptables策略才生效)
iptables -nvL --line-number

#删除规则
iptables -D RH-Firewall-1-INPUT 4
```

# 七、网络插件部署

1、master上部署flannel插件
```
#插件镜像 network: flannel image（因墙的问题，需要从国内源下载）
docker pull quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64
docker tag quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64  quay.io/coreos/flannel:v0.11.0-amd64

https://www.cnblogs.com/horizonli/p/10855666.html

#部署flannel
rm -f kube-flannel.yml
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl apply -f kube-flannel.yml

#另外需要注意的是如果你的节点有多个网卡的话，需要在 kube-flannel.yml 中使用--iface参数指定集群主机内网网卡的名称，否则可能会出现 dns 无法解析。flanneld 启动参数加上--iface=<iface-name>
args:
- --ip-masq
- --kube-subnet-mgr
- --iface=eth0
```

2、master上部署calico插件
```
export POD_SUBNET=10.244.0.0/16
rm -f calico.yaml
wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml
sed -i "s#192\.168\.0\.0/16#${POD_SUBNET}#" calico.yaml
kubectl apply -f calico.yaml

https://www.cnblogs.com/goldsunshine/p/10701242.html  k8s网络之Calico网络
```
3、性能对比
```
https://www.2cto.com/net/201701/591629.html  kubernetes flannel neutron calico三种网络方案性能测试分析
```
# 八、安装 Dashboard

使用 dashboard 最好把浏览器的默认语言设置为英文，不然在进入容器操作的时候会有bug，会出现重影

1、下载yaml文件
```
wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml

vim kubernetes-dashboard.yaml
1、# 修改镜像名称
......
    spec:
      containers:
      - name: kubernetes-dashboard
        #image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1 #这个换成阿里云的镜像
        image: registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1
        ports:
        - containerPort: 8443
          protocol: TCP
        args:
          - --auto-generate-certificates
......
2、# 修改Service为NodePort类型
......
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  type: NodePort   # 新增这一行，指定为NodePort方式
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 32370  #新增这一行，指定固定node端口
  selector:
    k8s-app: kubernetes-dashboard
```
2、dashboard最终文件
```
cat > kubernetes-dashboard.yaml << \EOF
# Copyright 2017 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# ------------------- Dashboard Secret ------------------- #

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-certs
  namespace: kube-system
type: Opaque

---
# ------------------- Dashboard Service Account ------------------- #

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system

---
# ------------------- Dashboard Role & Role Binding ------------------- #

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: kubernetes-dashboard-minimal
  namespace: kube-system
rules:
  # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret.
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["create"]
  # Allow Dashboard to create 'kubernetes-dashboard-settings' config map.
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["create"]
  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.
- apiGroups: [""]
  resources: ["secrets"]
  resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs"]
  verbs: ["get", "update", "delete"]
  # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["kubernetes-dashboard-settings"]
  verbs: ["get", "update"]
  # Allow Dashboard to get metrics from heapster.
- apiGroups: [""]
  resources: ["services"]
  resourceNames: ["heapster"]
  verbs: ["proxy"]
- apiGroups: [""]
  resources: ["services/proxy"]
  resourceNames: ["heapster", "http:heapster:", "https:heapster:"]
  verbs: ["get"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubernetes-dashboard-minimal
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubernetes-dashboard-minimal
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard
  namespace: kube-system

---
# ------------------- Dashboard Deployment ------------------- #

kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kubernetes-dashboard
  template:
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
    spec:
      containers:
      - name: kubernetes-dashboard
        #image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1
        image: registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1
        ports:
        - containerPort: 8443
          protocol: TCP
        args:
          - --auto-generate-certificates
          # Uncomment the following line to manually specify Kubernetes API server Host
          # If not specified, Dashboard will attempt to auto discover the API server and connect
          # to it. Uncomment only if the default does not work.
          # - --apiserver-host=http://my-address:port
        volumeMounts:
        - name: kubernetes-dashboard-certs
          mountPath: /certs
          # Create on-disk volume to store exec logs
        - mountPath: /tmp
          name: tmp-volume
        livenessProbe:
          httpGet:
            scheme: HTTPS
            path: /
            port: 8443
          initialDelaySeconds: 30
          timeoutSeconds: 30
      volumes:
      - name: kubernetes-dashboard-certs
        secret:
          secretName: kubernetes-dashboard-certs
      - name: tmp-volume
        emptyDir: {}
      serviceAccountName: kubernetes-dashboard
      # Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule

---
# ------------------- Dashboard Service ------------------- #

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  type: NodePort  # 新增这一行，指定为NodePort方式
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 32370  #新增这一行，指定固定node端口
  selector:
    k8s-app: kubernetes-dashboard
EOF

kubectl apply -f kubernetes-dashboard.yaml
```

3、查看dashboard
```
root># kubectl get pods -n kube-system -l k8s-app=kubernetes-dashboard
NAME                                  READY   STATUS    RESTARTS   AGE
kubernetes-dashboard-fcfb4cbc-dqbq9   1/1     Running   0          8m5s

root># kubectl get svc -n kube-system -l k8s-app=kubernetes-dashboard
NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   192.168.56.11   <none>        443:32730/TCP   8m25s

然后可以通过上面的 https://NodeIP:32730 端口去访问 Dashboard，要记住使用 https，Chrome不生效可以使用Firefox测试：
```

4、然后创建一个具有全局所有权限的用户来登录Dashboard：(admin.yaml)
```
cat > admin.yaml << \EOF
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: admin
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: admin
  namespace: kube-system

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin
  namespace: kube-system
  labels:
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
EOF

kubectl apply -f admin.yaml

kubectl delete -f admin.yaml

#获取token
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin | awk '{print $1}')
```

https://192.168.56.12:31513

然后用上面的base64解码后的字符串作为token登录Dashboard即可： k8s dashboard

最终我们就完成了使用 kubeadm 搭建 v1.15.3 版本的 kubernetes 集群、coredns、ipvs、flannel。 

# 九、问题排查

1、coredns异常问题

  ![coredns异常问题](https://github.com/Lancger/opsfull/blob/master/images/coredns-01.png)
  
```
E1006 12:30:53.935744       1 reflector.go:134] github.com/coredns/coredns/plugin/kubernetes/controller.go:317: Failed to list *v1.Endpoints: Get https://10.10.0.1:443/api/v1/endpoints?limit=500&resourceVersion=0: dial tcp 10.10.0.1:443: connect: no route to host
E1006 12:30:53.935744       1 reflector.go:134] github.com/coredns/coredns/plugin/kubernetes/controller.go:317: Failed to list *v1.Endpoints: Get https://10.10.0.1:443/api/v1/endpoints?limit=500&resourceVersion=0: dial tcp 10.10.0.1:443: connect: no route to host
log: exiting because of error: log: cannot create log: open /tmp/coredns.coredns-bccdc95cf-vlqxk.unknownuser.log.ERROR.20191006-123053.1: no such file or directory
```
解决办法
```
实际上是主机防火墙的问题，需要添加
iptables -A RH-Firewall-1-INPUT -s 10.10.0.0/16 -j ACCEPT

其他参考
https://medium.com/@cminion/quicknote-kubernetes-networking-issues-78f1e0d06e12
https://github.com/coredns/coredns/issues/2325  
```

2、kubelet异常问题1

```
问题现象：

kubelet fails to get cgroup stats for docker and kubelet services

解决办法:

cat > /etc/sysconfig/kubelet <<\EOF
KUBELET_EXTRA_ARGS=--runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice
EOF

systemctl daemon-reload
systemctl restart kubelet
systemctl status kubelet

#查看kubelet日志
journalctl -f -u kubelet

https://stackoverflow.com/questions/46726216/kubelet-fails-to-get-cgroup-stats-for-docker-and-kubelet-services  

https://www.twblogs.net/a/5cc87d63bd9eee1ac2ed736b
```

3、kubelet异常问题2
```
failed to create kubelet: misconfiguration: kubelet cgroup driver: "cgroupfs" is different from docker cgroup driver: "systemd"

#解决办法
添加如下内容--cgroup-driver=systemd

[root@tw19336 ~]# cat /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf
# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --cgroup-driver=systemd"
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
# This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/sysconfig/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS


systemctl daemon-reload
systemctl restart kubelet
systemctl status kubelet
https://www.cnblogs.com/hongdada/p/9771857.html
```

参考文档：

https://www.cnblogs.com/liyongjian5179/p/11417794.html   使用kubeadm安装Kubernetes 1.15.3 并开启 ipvs

https://www.jianshu.com/p/8bc61078bded 

https://www.cnblogs.com/lovesKey/p/10888006.html  centos7下用kubeadm安装k8s集群并使用ipvs做高可用方案

https://github.com/kubernetes/dashboard/wiki/Creating-sample-user

https://www.qikqiak.com/post/use-kubeadm-install-kubernetes-1.15.3/ 

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/  官方文档 

https://www.jianshu.com/p/d0933d6ae162 kubeadm 1.15 安装

https://yq.aliyun.com/articles/680080/  单独部署coredns

